{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715a499f",
   "metadata": {},
   "source": [
    "# 4-4A - Cloud and cloud-shadow masks pipeline for CBERS4A with four spectral bands\n",
    "Python notebook with the proposed pipeline for generating cloud and cloud shadow masks with the bands Blue, Green, Red and Nir. <br>\n",
    "Developed for Geospatial Data Science course, INPE (Instituto Nacional de Pesquisas Espaciais) <br>\n",
    "Author: JÃºlia Ascencio Cansado\n",
    "\n",
    "---\n",
    "\n",
    "## INTRODUCTION\n",
    "The notebook works by creating a local database, storing the informations / metadata needed to fetch the important bits!\n",
    "If you process multiple scenes, I would recommend filtering the database afterwards - that way you can get the images with the least amount of cloud / cloud shadow coverage, instead of all masks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255a683",
   "metadata": {},
   "source": [
    "### 1.DEFINE PARAMETERS\n",
    "This notebook processes images from CB4A-WPM-L4-DN-1 collection using BDC's (INPE) STAC service. <br>\n",
    "You can change these parameters, if you wish, but other modifications may be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef4b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE FREELY\n",
    "minx, miny, maxx, maxy = -53.248562,-19.498364,-45.907155,-12.394973\n",
    "bbox = f\"{minx}, {miny}, {maxx}, {maxy}\"\n",
    "datetime = '2025-01-01/2025-01-31'\n",
    "\n",
    "# THINK BEFORE YOU CHANGE\n",
    "stac = \"https://data.inpe.br/bdc/stac/v1/\"\n",
    "collection_id = \"CB4A-WPM-L4-DN-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90be5a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85ec36",
   "metadata": {},
   "source": [
    "### 2. AUXILIARY FUNCTIONS\n",
    "These functions are called inside the main function, but you can edit it! <br>\n",
    "Remove and add indexes and operations as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60289eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "import ast\n",
    "import zlib\n",
    "import json\n",
    "import requests\n",
    "import pystac_client\n",
    "import sqlite3\n",
    "import rasterio \n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from rasterio.transform import Affine\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc28964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING FUNCTIONS \n",
    "def get_coefficient(assets):\n",
    "    NS = '{http://www.gisplan.com.br/xmlsat}'\n",
    "    response = requests.get(assets['BAND1_xml'].href)\n",
    "    response.raise_for_status()\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    image_element = root.find(f\"{NS}image\")\n",
    "    if image_element is not None:\n",
    "        calibration_element = image_element.find(f\"{NS}absoluteCalibrationCoefficient\")\n",
    "        if calibration_element is not None:\n",
    "            calibration_data = {}\n",
    "            for band_data in calibration_element:\n",
    "                clean_tag = band_data.tag.replace(NS, '')\n",
    "                \n",
    "                band_id = band_data.attrib.get('name', 'N/A')\n",
    "                band_value = band_data.text.strip() if band_data.text else 'N/A'\n",
    "                \n",
    "                key = f\"{clean_tag} {band_id}\".strip() \n",
    "                \n",
    "                calibration_data[key] = float(band_value)\n",
    "        else:\n",
    "            calibration_data = {'band 0': 0.184471, 'band 1': 0.29107, 'band 2': 0.297832, 'band 3': 0.232504, 'band 4': 0.178993}\n",
    "\n",
    "    return calibration_data\n",
    "\n",
    "def normalize(array):\n",
    "    \"\"\"Normalizes numpy arrays into scale 0.0 - 1.0\"\"\"\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return ((array - array_min)/(array_max - array_min))\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# STATISTICS AND INDEXES FUNCTIONS\n",
    "def estatisticas(array):\n",
    "    dados_flat = array.flatten()\n",
    "    dados_validos = dados_flat[dados_flat != 0]\n",
    "    dados_validos = dados_validos[~np.isnan(dados_validos)]\n",
    "    media = np.mean(dados_validos)\n",
    "    mediana = np.median(dados_validos)\n",
    "    minimo = np.min(dados_validos)\n",
    "    maximo = np.max(dados_validos)\n",
    "    desvio_padrao = np.std(dados_validos)\n",
    "    \n",
    "    q25 = np.percentile(dados_validos, 25)\n",
    "    q75 = np.percentile(dados_validos, 75)\n",
    "    \n",
    "    stats = {'media': media,\n",
    "             'mediana': mediana,\n",
    "             'minimo': minimo,\n",
    "             'maximo': maximo,\n",
    "             'std': desvio_padrao,\n",
    "             'q25': q25,\n",
    "             'q75': q75}\n",
    "    return stats\n",
    "\n",
    "def get_NDVI(band3, band4, footprint):\n",
    "    ndvi = (((band4 - band3)/(band4 + band3)) * (footprint))\n",
    "    ndvi_stats = estatisticas(ndvi)\n",
    "    ndvi_binary = (((ndvi_stats['minimo'] + ndvi_stats['std']) < ndvi) & (ndvi < (ndvi_stats['mediana'] - ndvi_stats['std']))) * footprint\n",
    "    return ndvi_binary, ndvi_stats, ndvi\n",
    "\n",
    "def get_WI(band1, band2, band3, footprint):\n",
    "    m = (0.25 * band1) + (0.375 * band2) + (0.375 * band3)\n",
    "    wi = abs((band1 - m) / m) + abs((band2 - m) / m) + abs((band3 - m) / m)\n",
    "    wi_stats = estatisticas(wi)\n",
    "    wi_binary = (wi < wi_stats['q25']) * footprint\n",
    "    return wi_binary, wi_stats\n",
    "\n",
    "def get_HOT(band1, band3, footprint):\n",
    "    hot = band1 - (0.45 * band3) - 0.08\n",
    "    hot_stats = estatisticas(hot)\n",
    "    hot_binary = (hot > hot_stats['q75']) * footprint\n",
    "    return hot_binary, hot_stats\n",
    "\n",
    "def get_CI(band1, band2, band3, band4, footprint):\n",
    "    # cloud index\n",
    "    ci = ((3 * band4) / (band1 + band2 + band3)) * footprint\n",
    "    ci_stats = estatisticas(ci)\n",
    "    ci_binary = ci < ci_stats['q25'] \n",
    "    return ci_binary, ci_stats\n",
    "\n",
    "def get_NSCD(band1, band3, band4, footprint):\n",
    "    nscd = band1 - (0.45 * band3) - (0.16 * band4) * footprint\n",
    "    nscd_stats = estatisticas(nscd)\n",
    "    nscd_binary = nscd > nscd_stats['q75'] \n",
    "    return nscd_binary, nscd_stats\n",
    "\n",
    "def get_D(band2, band4, footprint):\n",
    "    band2_stats = estatisticas(band2)\n",
    "    band4_stats = estatisticas(band4)\n",
    "    D_binary = ((band2 < band2_stats['q25'] + band2_stats['std']) & (band4 < band4_stats['minimo'] + band4_stats['std'])) * footprint\n",
    "    return D_binary, band2_stats, band4_stats\n",
    "\n",
    "def get_W(ndvi, band4, footprint):\n",
    "    ndvi_stats = estatisticas(ndvi)\n",
    "    NDVI_clean =  ndvi_stats['minimo'] + (1.5*ndvi_stats['std'])\n",
    "    band4_clean = ndvi_stats['q75'] + ndvi_stats['std'] \n",
    "    NDVI_turbid =   ndvi_stats['minimo'] + (2 * ndvi_stats['std'])\n",
    "    band4_turbid = ndvi_stats['q25'] + ndvi_stats['std'] \n",
    "    w_clean = (ndvi < NDVI_clean) & (band4 < band4_clean)\n",
    "    w_turbid = (ndvi < NDVI_turbid) & (band4 < band4_turbid)\n",
    "    W_binary = (w_clean | w_turbid) * footprint\n",
    "    return W_binary\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# METADATA RELATED FUNCTIONS\n",
    "def compress_array_to_blob(arr):\n",
    "    buffer = io.BytesIO()\n",
    "    np.save(buffer, arr)\n",
    "    uncompressed_blob = buffer.getvalue()\n",
    "\n",
    "    compressed_blob = zlib.compress(uncompressed_blob)\n",
    "    return compressed_blob\n",
    "#--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dec4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b78c6",
   "metadata": {},
   "source": [
    "### 3. MAIN PROCESSING FUNCTION\n",
    "Here is the main function, it reads the necessary bands, calculates indexes, image footprint, cloud cover and etc., updating the image's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e7f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(id):\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    print(f\"Processing item: {id}\")\n",
    "    try:\n",
    "        item = next(service.get_items(id))\n",
    "        assets = item.assets\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Service access failed for item {id}: {e}\")\n",
    "        return {'id': id, 'corrupt': True, 'error_type': 'ServiceAccessError'}\n",
    "\n",
    "    bands_to_read = ['BAND1', 'BAND2', 'BAND3', 'BAND4']\n",
    "    item_bands = {}\n",
    "    original_crs = None\n",
    "    original_transform = None\n",
    "    \n",
    "    # READ BANDS\n",
    "    print(f\"Opening {id} bands\")\n",
    "    try:\n",
    "        for name, asset in assets.items():\n",
    "            if name in bands_to_read:\n",
    "                with rasterio.open(assets[name].href) as src:\n",
    "                    item_bands[name] = src.read(1)\n",
    "\n",
    "                    if original_crs is None:\n",
    "                        original_crs = src.crs.to_wkt()                \n",
    "                        original_transform = src.transform.to_gdal()\n",
    "        \n",
    "        if len(item_bands) != len(bands_to_read):\n",
    "            raise IOError(f\"Missing one or more required bands: {bands_to_read}\")\n",
    "\n",
    "    except (rasterio.RasterioIOError, IOError, ValueError) as e:\n",
    "        print(f\"FATAL ERROR: Band reading failed for item {id}. Details: {e}\")\n",
    "        return {\n",
    "            'id': id, \n",
    "            'collection_id': item.collection_id, \n",
    "            'corrupt': True, \n",
    "            'error_type': 'BandReadError',\n",
    "            'error_detail': str(e)\n",
    "        }\n",
    "        \n",
    "    print(f\"Finished opening {id} bands\")\n",
    "\n",
    "    # CALIBRATE / NORMALIZE BANDS\n",
    "    calibration_data = get_coefficient(assets)\n",
    "\n",
    "    band1 = normalize(item_bands['BAND1'] * calibration_data['band 1'])\n",
    "    band2 = normalize(item_bands['BAND2'] * calibration_data['band 2'])\n",
    "    band3 = normalize(item_bands['BAND3'] * calibration_data['band 3'])\n",
    "    band4 = normalize(item_bands['BAND4'] * calibration_data['band 4'])\n",
    "    \n",
    "    # GET IMAGE BORDERS / PIXELS\n",
    "    band_mismatch = np.sum(((band1 != 0)^ (band2 != 0)) | ((band2 != 0) ^ (band3 != 0)) | ((band1 != 0) ^ (band3 != 0))) > 100\n",
    "    border = (band1 == 0) | (band2 == 0) | (band3 == 0) | (band4 == 0) \n",
    "    footprint = ~ border\n",
    "    total_pixels = np.sum(~ border)\n",
    "    \n",
    "    # CLOUD MASK\n",
    "    print(f\"Start {id} cloud mask\")\n",
    "\n",
    "    # ndvi\n",
    "    ndvi_binary, ndvi_stats, ndvi = get_NDVI(band3, band4, footprint)\n",
    "    #whiteness index\n",
    "    wi_binary, wi_stats = get_WI(band1, band2, band3, footprint)\n",
    "    # hot\n",
    "    hot_binary, hot_stats = get_HOT(band1, band3, footprint)\n",
    "    # cloud index\n",
    "    ci_binary, ci_stats = get_CI(band1, band2, band3, band4, footprint)\n",
    "    # nscd\n",
    "    nscd_binary, nscd_stats = get_NSCD(band1, band3, band4, footprint)\n",
    "\n",
    "    cloud =  ndvi_binary & wi_binary & hot_binary & ci_binary & nscd_binary\n",
    "    cloud_percentage = (np.sum(cloud) / total_pixels) * 100\n",
    "\n",
    "    # CLOUD SHADOW MASK\n",
    "    print(f\"Start {id} cloud shadow mask\")\n",
    "    # D\n",
    "    D_binary, band2_stats, band4_stats = get_D(band2, band4, footprint)\n",
    "    # W\n",
    "    W_binary = get_W(ndvi, band4, footprint)\n",
    "    W_percentage = (np.sum(W_binary) / total_pixels) * 100\n",
    "\n",
    "    cloud_shadow = D_binary & (~ W_binary)\n",
    "    cloud_shadow_percentage = (np.sum(cloud_shadow) / total_pixels) * 100\n",
    "\n",
    "    # MERGE MASKS AND SAVE TO BLOB\n",
    "    print(f\"Merging masks and saving to blob\")\n",
    "    merged_mask = np.zeros_like(cloud, dtype=np.uint16)\n",
    "    merged_mask = np.where(cloud == 1, 200, merged_mask)\n",
    "    merged_mask = np.where(cloud_shadow == 1, 100, merged_mask)\n",
    "\n",
    "    raster_blob = compress_array_to_blob(merged_mask)\n",
    "\n",
    "    dictionary = item.properties\n",
    "    dictionary['raster_blob'] = raster_blob\n",
    "\n",
    "    # METADATA DICTIONARY UPDATE\n",
    "    print(f\"Updating {id} dictionary\")\n",
    "    dictionary['id'] = item.id\n",
    "    dictionary['collection_id'] = item.collection_id\n",
    "    del dictionary['eo:cloud_cover']\n",
    "    dictionary['path'] = 'path'\n",
    "    del dictionary['path']\n",
    "    dictionary['row'] = 'row'\n",
    "    del dictionary['row']\n",
    "    dictionary['band_mismatch'] = band_mismatch.item()\n",
    "    dictionary['cloud_percentage'] = cloud_percentage.item()\n",
    "    dictionary['water_percentage'] = W_percentage.item()\n",
    "    dictionary['cloud_shadow_percentage'] = cloud_shadow_percentage.item()\n",
    "    transform_string = json.dumps(original_transform)\n",
    "    dictionary['crs_wkt'] =  original_crs\n",
    "    dictionary['transform_gdal'] = transform_string\n",
    "    dictionary['corrupt'] = False\n",
    "\n",
    "    print(f\"Finished processing item: {id}\")\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684df18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb10de",
   "metadata": {},
   "source": [
    "### 4. SETTING UP LOCAL DATABASE\n",
    "Functions to create table / insert data / check if a scene has already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fdeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(db_name, table_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            collection_id TEXT,\n",
    "            created TEXT,\n",
    "            start_datetime TEXT,\n",
    "            end_datetime TEXT,\n",
    "            datetime TEXT,\n",
    "            updated TEXT,\n",
    "            crs_wkt TEXT,\n",
    "            transform_gdal TEXT,\n",
    "            band_mismatch BOOLEAN,\n",
    "            cloud_percentage FLOAT,\n",
    "            cloud_shadow_percentage FLOAT,\n",
    "            water_percentage FLOAT,\n",
    "            raster_blob BLOB,\n",
    "            row INT,\n",
    "            corrupt BOOLEAN\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_image_data(db_name: str, table_name: str, data_dict: dict):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        quoted_columns = ', '.join(f'\"{key}\"' for key in data_dict.keys())\n",
    "        \n",
    "        placeholders = ', '.join(['?'] * len(data_dict))\n",
    "        values = tuple(data_dict.values())\n",
    "\n",
    "        sql_insert = f\"INSERT INTO {table_name} ({quoted_columns}) VALUES ({placeholders})\"\n",
    "  \n",
    "        cursor.execute(sql_insert, values)\n",
    "        conn.commit()\n",
    "        print(f\"Successfully inserted record with ID: {data_dict.get('id')}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error occurred: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def check_db_for_id(db_name: str, table_name: str, image_id: str):\n",
    "    \"\"\"\n",
    "    Check if id exists in db.\n",
    "    \n",
    "    Args:\n",
    "        db_name: The database file name.\n",
    "        table_name: The table name.\n",
    "        image_id: The string ID of the record to retrieve.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT collection_id\n",
    "            FROM {table_name}\n",
    "            WHERE id = ?\n",
    "        \"\"\", (image_id,)) # <-- Pass the ID as a single-element tuple for safety\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if not result:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error during check: {e}\")\n",
    "        return False # Return False on error\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95cfef",
   "metadata": {},
   "source": [
    "Create table if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5412907",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = 'cd_geo_github.db'\n",
    "TABLE_NAME = 'cloud_processing_github'\n",
    "\n",
    "create_table(DB_FILE, TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bd17e",
   "metadata": {},
   "source": [
    "Code snippet to drop table, if you wish to start from scratch (uncomment it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e85db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fb9ad853a40>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection = sqlite3.connect(DB_FILE)\n",
    "# connection.execute(f'DROP TABLE {TABLE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05193d72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a15d0",
   "metadata": {},
   "source": [
    "### 5. RUN CODE\n",
    "Fetch items that match your parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20e0317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = pystac_client.Client.open(stac)\n",
    "item_search = service.search(bbox= bbox,\n",
    "                             datetime=datetime,\n",
    "                             collections=[collection_id])\n",
    "item_search.matched()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2175002",
   "metadata": {},
   "source": [
    "If you wish to run one by one (or you do not have a powerful computer), uncomment the code snippet below. <br>\n",
    "This method also has the advantage of inserting the data after processing each item - if it fails along the way, you will still have data in you database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46f82ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item: CBERS_4A_WPM_20250129_215_128_L4\n",
      "Opening CBERS_4A_WPM_20250129_215_128_L4 bands\n",
      "Finished opening CBERS_4A_WPM_20250129_215_128_L4 bands\n",
      "Start CBERS_4A_WPM_20250129_215_128_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250129_215_128_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250129_215_128_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250129_215_128_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250129_215_128_L4\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Processing item: CBERS_4A_WPM_20250122_204_136_L4\n",
      "Opening CBERS_4A_WPM_20250122_204_136_L4 bands\n",
      "Finished opening CBERS_4A_WPM_20250122_204_136_L4 bands\n",
      "Start CBERS_4A_WPM_20250122_204_136_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250122_204_136_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250122_204_136_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250122_204_136_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250122_204_136_L4\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Processing item: CBERS_4A_WPM_20250122_204_135_L4\n",
      "Opening CBERS_4A_WPM_20250122_204_135_L4 bands\n",
      "Finished opening CBERS_4A_WPM_20250122_204_135_L4 bands\n",
      "Start CBERS_4A_WPM_20250122_204_135_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250122_204_135_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250122_204_135_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250122_204_135_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250122_204_135_L4\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MAX_ITEMS_TO_PROCESS = 4\n",
    "count = 0\n",
    "processed_data_dicts = []\n",
    "\n",
    "for item in item_search.items():\n",
    "    item_id = item.id\n",
    "    count += 1\n",
    "\n",
    "    if count >= MAX_ITEMS_TO_PROCESS:\n",
    "        break \n",
    "    if check_db_for_id(DB_FILE, TABLE_NAME, item_id):\n",
    "        print(f\"Item {item_id} already processed. Skipping.\")\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        continue\n",
    "    else:\n",
    "        # print(f\"Processing new item: {item_id}\")\n",
    "        data_dict = process_item(item_id) \n",
    "        insert_image_data(DB_FILE, TABLE_NAME, data_dict)\n",
    "        print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b508b4",
   "metadata": {},
   "source": [
    "If you have a better computer, you can try running multiple images in parallel, as the cell below. <br>\n",
    "This is much faster, but the insertion happens only after everything is processed - if something happens along the way, you might have to reprocess everything :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2015d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item CBERS_4A_WPM_20250129_215_128_L4 already processed. Skipping.Item CBERS_4A_WPM_20250122_204_136_L4 already processed. Skipping.Item CBERS_4A_WPM_20250122_204_135_L4 already processed. Skipping.\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Processing item: CBERS_4A_WPM_20250118_211_128_L4Processing item: CBERS_4A_WPM_20250122_204_134_L4Processing item: CBERS_4A_WPM_20250112_206_137_L4\n",
      "\n",
      "\n",
      "ERROR: Service access failed for item CBERS_4A_WPM_20250122_204_134_L4: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Processing item: CBERS_4A_WPM_20250107_207_130_L4\n",
      "Opening CBERS_4A_WPM_20250118_211_128_L4 bandsOpening CBERS_4A_WPM_20250107_207_130_L4 bands\n",
      "\n",
      "Opening CBERS_4A_WPM_20250112_206_137_L4 bands\n",
      "Finished opening CBERS_4A_WPM_20250112_206_137_L4 bands\n",
      "Start CBERS_4A_WPM_20250112_206_137_L4 cloud mask\n",
      "Finished opening CBERS_4A_WPM_20250118_211_128_L4 bands\n",
      "Finished opening CBERS_4A_WPM_20250107_207_130_L4 bands\n",
      "Start CBERS_4A_WPM_20250107_207_130_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250118_211_128_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250107_207_130_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Start CBERS_4A_WPM_20250118_211_128_L4 cloud shadow mask\n",
      "Updating CBERS_4A_WPM_20250107_207_130_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250107_207_130_L4\n",
      "Processing item: CBERS_4A_WPM_20250103_214_129_L4\n",
      "Opening CBERS_4A_WPM_20250103_214_129_L4 bands\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250118_211_128_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250118_211_128_L4\n",
      "Start CBERS_4A_WPM_20250112_206_137_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250112_206_137_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250112_206_137_L4\n",
      "Finished opening CBERS_4A_WPM_20250103_214_129_L4 bands\n",
      "Start CBERS_4A_WPM_20250103_214_129_L4 cloud mask\n",
      "Start CBERS_4A_WPM_20250103_214_129_L4 cloud shadow mask\n",
      "Merging masks and saving to blob\n",
      "Updating CBERS_4A_WPM_20250103_214_129_L4 dictionary\n",
      "Finished processing item: CBERS_4A_WPM_20250103_214_129_L4\n",
      "--- 5 items to process. ---\n",
      "SQLite error occurred: table cloud_processing_github has no column named error_type\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250118_211_128_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250112_206_137_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250107_207_130_L4\n",
      "Successfully inserted record with ID: CBERS_4A_WPM_20250103_214_129_L4\n"
     ]
    }
   ],
   "source": [
    "MAX_ITEMS_TO_PROCESS = 8 \n",
    "MAX_WORKERS = 3 \n",
    "\n",
    "def process_items(item):\n",
    "    \"\"\"Same function structure, but executed concurrently by threads.\"\"\"\n",
    "    item_id = item.id\n",
    "    if check_db_for_id(DB_FILE, TABLE_NAME, item_id):\n",
    "        print(f\"Item {item_id} already processed. Skipping.\")\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        return None\n",
    "    else:\n",
    "        data_dict = process_item(item_id) \n",
    "        return data_dict\n",
    "        print('--------------------------------------------------------------------------')\n",
    "\n",
    "processed_data_dicts = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    limited_items = itertools.islice(item_search.items(), MAX_ITEMS_TO_PROCESS)\n",
    "    results = executor.map(process_items,limited_items)\n",
    "    processed_data_dicts = [d for d in results if d is not None]\n",
    "\n",
    "print(f\"--- {len(processed_data_dicts)} items to process. ---\")\n",
    "\n",
    "for data_dict in processed_data_dicts:\n",
    "    insert_image_data(DB_FILE, TABLE_NAME, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ed6a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85128389",
   "metadata": {},
   "source": [
    "### 6. RETRIEVE RASTER FROM DB\n",
    "After processing the desired images, you can filter the ids you want by the metadata you generated! And then retrieve and transform the masks into .tiff files to apply it in QGIS or other platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94301f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "MAX_CLOUD_PERCENTAGE = 15 #Filtering by cloud percentage\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = f\"SELECT * FROM cloud_processing_github WHERE cloud_percentage < ?\"\n",
    "\n",
    "cursor.execute(query, (MAX_CLOUD_PERCENTAGE,))\n",
    "rows = cursor.fetchall()\n",
    "id_list = [row[0] for row in rows]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c39d1",
   "metadata": {},
   "source": [
    "Define function to get raster from the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f4a3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raster_by_id(db_name: str, table_name: str, image_id: str):\n",
    "    \"\"\"\n",
    "    Retrieves the raster array and metadata for a specific image_id.\n",
    "    \n",
    "    Args:\n",
    "        db_name: The database file name.\n",
    "        table_name: The table name.\n",
    "        image_id: The string ID of the record to retrieve.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "        clean_id = image_id.strip()\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT raster_blob, crs_wkt, transform_gdal \n",
    "            FROM {table_name}\n",
    "            WHERE id = ?\n",
    "        \"\"\", (clean_id,)) \n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"Error: No data found for ID: {image_id}.\")\n",
    "            return None, None, None\n",
    "\n",
    "        raster_blob, crs_wkt, transform_gdal_str = result\n",
    "        retrieved_array = np.load(BytesIO(zlib.decompress(raster_blob)))\n",
    "        transform_tuple = ast.literal_eval(transform_gdal_str) \n",
    "        retrieved_transform = Affine.from_gdal(*transform_tuple) \n",
    "\n",
    "        return retrieved_array, crs_wkt, retrieved_transform\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error occurred: {e}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during reconstruction: {e}\")\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac831a2d",
   "metadata": {},
   "source": [
    "Save masks into .tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a037a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved array for ID: CBERS_4A_WPM_20250122_204_136_L4\n",
      "\n",
      "Saved georeferenced raster to ./masks_github/CBERS_4A_WPM_20250122_204_136_L4_mask.tif\n",
      "------------------------------------------------------------\n",
      "Successfully retrieved array for ID: CBERS_4A_WPM_20250122_204_135_L4\n",
      "\n",
      "Saved georeferenced raster to ./masks_github/CBERS_4A_WPM_20250122_204_135_L4_mask.tif\n",
      "------------------------------------------------------------\n",
      "Successfully retrieved array for ID: CBERS_4A_WPM_20250118_211_128_L4\n",
      "\n",
      "Saved georeferenced raster to ./masks_github/CBERS_4A_WPM_20250118_211_128_L4_mask.tif\n",
      "------------------------------------------------------------\n",
      "Successfully retrieved array for ID: CBERS_4A_WPM_20250112_206_137_L4\n",
      "\n",
      "Saved georeferenced raster to ./masks_github/CBERS_4A_WPM_20250112_206_137_L4_mask.tif\n",
      "------------------------------------------------------------\n",
      "Successfully retrieved array for ID: CBERS_4A_WPM_20250107_207_130_L4\n",
      "\n",
      "Saved georeferenced raster to ./masks_github/CBERS_4A_WPM_20250107_207_130_L4_mask.tif\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in id_list:\n",
    "    target_id = item\n",
    "    OUTPUT_FILENAME =  os.path.join('./masks_github',f'{target_id}_mask.tif')\n",
    "    raster_data, crs_wkt, transform = get_raster_by_id(DB_FILE, TABLE_NAME, target_id)\n",
    "\n",
    "    if raster_data is not None:\n",
    "        print(f\"Successfully retrieved array for ID: {target_id}\")\n",
    "        height, width = raster_data.shape\n",
    "        \n",
    "        profile = {\n",
    "            'driver': 'GTiff',\n",
    "            'dtype': raster_data.dtype,\n",
    "            'count': 1,           \n",
    "            'height': height,\n",
    "            'width': width,\n",
    "            'crs': crs_wkt,           \n",
    "            'transform': transform,  \n",
    "            'nodata': 0,         \n",
    "            'compress': 'lzw'    \n",
    "        }\n",
    "\n",
    "        # Open the output file for writing\n",
    "        with rasterio.open(OUTPUT_FILENAME, 'w', **profile) as dst:\n",
    "            dst.write(raster_data, 1)\n",
    "\n",
    "        print(f\"\\nSaved georeferenced raster to {OUTPUT_FILENAME}\")\n",
    "        print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b59b0",
   "metadata": {},
   "source": [
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
